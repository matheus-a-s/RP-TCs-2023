{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ddbc8229-be15-454d-9dfd-27fac1eb0eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(310, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 63.03,  22.55,  39.61,  40.48,  98.67,  -0.25],\n",
       "       [ 39.06,  10.06,  25.02,  29.  , 114.41,   4.56],\n",
       "       [ 68.83,  22.22,  50.09,  46.61, 105.99,  -3.53],\n",
       "       ...,\n",
       "       [ 61.45,  22.69,  46.17,  38.75, 125.67,  -2.71],\n",
       "       [ 45.25,   8.69,  41.58,  36.56, 118.55,   0.21],\n",
       "       [ 33.84,   5.07,  36.64,  28.77, 123.95,  -0.2 ]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "data_path = \"./dataset/datasetTC4.dat\"\n",
    "\n",
    "data = np.loadtxt(data_path)\n",
    "print(data.shape)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "431495ee-9c41-455a-9c7f-55b670ac5bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.14722652,  0.50111133, -0.66512805, -0.18460234, -1.44783071,\n",
       "        -0.70794606],\n",
       "       [-1.24570706, -0.74889057, -1.45276272, -1.04124965, -0.26402779,\n",
       "        -0.57967342],\n",
       "       [ 0.48427345,  0.46808485, -0.0993699 ,  0.27282344, -0.89729467,\n",
       "        -0.79541679],\n",
       "       ...,\n",
       "       [ 0.05541029,  0.51512256, -0.31098936, -0.31369641,  0.58283504,\n",
       "        -0.77354911],\n",
       "       [-0.88599664, -0.88600047, -0.55877847, -0.47711606,  0.04734096,\n",
       "        -0.69567882],\n",
       "       [-1.54904929, -1.24829085, -0.82546218, -1.05841244,  0.45347411,\n",
       "        -0.70661266]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalização do dataset usando z-score\n",
    "def z_score_norm(dataset):\n",
    "    std = np.std(dataset, axis=0)\n",
    "    normalized = (dataset - np.mean(dataset, axis=0)) / np.where(std == 0, 1e-9, std) # Caso divisão por zero\n",
    "    return normalized\n",
    "\n",
    "data = z_score_norm(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b85eb0ca-710d-41f4-8a03-6fcf374f6041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialização dos parâmetros\n",
    "Kmax = 10 # Número de protótipos\n",
    "rounds = 30 # Número de rodadas\n",
    "epochs = 20 # Número de iterações\n",
    "ref_index = \"Dunn\" # Índice de referência para escolha do K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c7fb0591-6a21-427c-8e72-5b66c3668b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementação do Índice de Dunn, oss outros índices foram usados do Scikit-learn\n",
    "def dunn_index(data, labels):\n",
    "    min_intercluster_dists = np.max(pairwise_distances(data, metric='euclidean'))\n",
    "    max_intracluster_dists = 0.0\n",
    "    \n",
    "    for cluster_label in np.unique(labels):\n",
    "        cluster_points = data[labels == cluster_label]\n",
    "        \n",
    "        if cluster_label < len(np.unique(labels)) - 1:\n",
    "            not_cluster_points = data[labels > cluster_label]\n",
    "            intercluster_dists = pairwise_distances(cluster_points, not_cluster_points, metric='euclidean')\n",
    "            min_intercluster_dists = min(np.min(intercluster_dists), min_intercluster_dists)\n",
    "            \n",
    "        intracluster_dists = pairwise_distances(cluster_points, metric='euclidean')\n",
    "        max_intracluster_dists = max(np.max(intracluster_dists), max_intracluster_dists)\n",
    "\n",
    "    return min_intercluster_dists / max_intracluster_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "275eac67-390e-4025-bc07-953ee1d7893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch K-means\n",
    "def kmeans(data, proto, epochs):\n",
    "    N, p = data.shape\n",
    "    K = proto.shape[0]\n",
    "    SSD = np.zeros(epochs)\n",
    "\n",
    "    updt_proto = np.copy(proto)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        dist = np.zeros((N, K))\n",
    "        for t in range(N):\n",
    "            dist[t, :] = np.linalg.norm(data[t, :] - updt_proto, axis=1)\n",
    "        \n",
    "        cluster = np.argmin(dist, axis=1)\n",
    "        SSD[epoch] = np.sum(np.min(dist, axis=1)**2)\n",
    "\n",
    "        for k in range(K):\n",
    "            I = np.where(cluster == k)[0]\n",
    "            partition = data[I, :]\n",
    "            updt_proto[k, :] = np.mean(partition, axis=0)\n",
    "\n",
    "    return updt_proto, SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e6b77762-bed6-433c-8b59-d43a7babe310",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1/100 finished\n",
      "Round 2/100 finished\n",
      "Round 3/100 finished\n",
      "Round 4/100 finished\n",
      "Round 5/100 finished\n",
      "Round 6/100 finished\n",
      "Round 7/100 finished\n",
      "Round 8/100 finished\n",
      "Round 9/100 finished\n",
      "Round 10/100 finished\n",
      "Round 11/100 finished\n",
      "Round 12/100 finished\n",
      "Round 13/100 finished\n",
      "Round 14/100 finished\n",
      "Round 15/100 finished\n",
      "Round 16/100 finished\n",
      "Round 17/100 finished\n",
      "Round 18/100 finished\n",
      "Round 19/100 finished\n",
      "Round 20/100 finished\n",
      "Round 21/100 finished\n",
      "Round 22/100 finished\n",
      "Round 23/100 finished\n",
      "Round 24/100 finished\n",
      "Round 25/100 finished\n",
      "Round 26/100 finished\n",
      "Round 27/100 finished\n",
      "Round 28/100 finished\n",
      "Round 29/100 finished\n",
      "Round 30/100 finished\n",
      "Round 31/100 finished\n",
      "Round 32/100 finished\n",
      "Round 33/100 finished\n",
      "Round 34/100 finished\n",
      "Round 35/100 finished\n",
      "Round 36/100 finished\n",
      "Round 37/100 finished\n",
      "Round 38/100 finished\n",
      "Round 39/100 finished\n",
      "Round 40/100 finished\n",
      "Round 41/100 finished\n",
      "Round 42/100 finished\n",
      "Round 43/100 finished\n",
      "Round 44/100 finished\n",
      "Round 45/100 finished\n",
      "Round 46/100 finished\n",
      "Round 47/100 finished\n",
      "Round 48/100 finished\n",
      "Round 49/100 finished\n",
      "Round 50/100 finished\n",
      "Round 51/100 finished\n",
      "Round 52/100 finished\n",
      "Round 53/100 finished\n",
      "Round 54/100 finished\n",
      "Round 55/100 finished\n",
      "Round 56/100 finished\n",
      "Round 57/100 finished\n",
      "Round 58/100 finished\n",
      "Round 59/100 finished\n",
      "Round 60/100 finished\n",
      "Round 61/100 finished\n",
      "Round 62/100 finished\n",
      "Round 63/100 finished\n",
      "Round 64/100 finished\n",
      "Round 65/100 finished\n",
      "Round 66/100 finished\n",
      "Round 67/100 finished\n",
      "Round 68/100 finished\n",
      "Round 69/100 finished\n",
      "Round 70/100 finished\n",
      "Round 71/100 finished\n",
      "Round 72/100 finished\n",
      "Round 73/100 finished\n",
      "Round 74/100 finished\n",
      "Round 75/100 finished\n",
      "Round 76/100 finished\n",
      "Round 77/100 finished\n",
      "Round 78/100 finished\n",
      "Round 79/100 finished\n",
      "Round 80/100 finished\n",
      "Round 81/100 finished\n",
      "Round 82/100 finished\n",
      "Round 83/100 finished\n",
      "Round 84/100 finished\n",
      "Round 85/100 finished\n",
      "Round 86/100 finished\n",
      "Round 87/100 finished\n",
      "Round 88/100 finished\n",
      "Round 89/100 finished\n",
      "Round 90/100 finished\n",
      "Round 91/100 finished\n",
      "Round 92/100 finished\n",
      "Round 93/100 finished\n",
      "Round 94/100 finished\n",
      "Round 95/100 finished\n",
      "Round 96/100 finished\n",
      "Round 97/100 finished\n",
      "Round 98/100 finished\n",
      "Round 99/100 finished\n",
      "Round 100/100 finished\n",
      "Best K per Round by Dunn Index: {5: 5, 6: 24, 7: 13, 8: 15, 9: 20, 10: 23}\n",
      "Best K per Round by Calinski-Harabasz Index: {2: 100}\n",
      "Best K per Round by Davies-Bouldin Index: {2: 60, 4: 29, 5: 10, 7: 1}\n"
     ]
    }
   ],
   "source": [
    "# Rodada prévia para encontrar o melhor valor de K por cada índice\n",
    "k_by_dunn, k_by_ch, k_by_db = [], [], []\n",
    "\n",
    "for ext_rnd in range(100):\n",
    "    best_dunns, best_chs, best_dbs = [], [], []\n",
    "    \n",
    "    for K in range(2, Kmax+1):\n",
    "        rnd_final_prototypes, rnd_ssds = [], []\n",
    "        \n",
    "        for rnd in range(10): # Aplicando K-médias nos dados por rodada\n",
    "            idxs = np.random.choice(data.shape[0], K, replace=False) # Aleatoriedade dos protótipos\n",
    "            prototypes = data[idxs, :] # Protótipos definidos\n",
    "            \n",
    "            prototypes, kmeans_SSD = kmeans(data, prototypes, epochs) # Aplicando o K-means\n",
    "            \n",
    "            rnd_final_prototypes.append(prototypes)\n",
    "            rnd_ssds.append(kmeans_SSD)\n",
    "        \n",
    "        k_best_ssd_idx = np.argmin([ssd[-1] for ssd in rnd_ssds]) # Pega o índice da rodada com menor SSD final\n",
    "        k_best_final_prototypes = rnd_final_prototypes[k_best_ssd_idx]\n",
    "    \n",
    "        labels = np.argmin(cdist(data, k_best_final_prototypes), axis=1) # Cálcula as labels (clusters) de cada amostra\n",
    "    \n",
    "        best_dunns.append(dunn_index(data, labels)) # Salvando o Índice de Dunn de K\n",
    "        best_chs.append(calinski_harabasz_score(data, labels)) # Salvando o Índice de Calinski-Harabasz de K\n",
    "        best_dbs.append(davies_bouldin_score(data, labels)) # Salvando o Índice de Davies-Bouldin de K\n",
    "    \n",
    "    k_by_dunn.append(np.argmax(best_dunns) + 2) # Melhor K segundo o Índice de Dunn\n",
    "    k_by_ch.append(np.argmax(best_chs) + 2) # Melhor K segundo o Índice de Calinski-Harabasz\n",
    "    k_by_db.append(np.argmin(best_dbs) + 2) # Melhor K segundo o Índice de Davies-Bouldin\n",
    "    \n",
    "    print(\"Round {}/100 finished\".format(ext_rnd + 1))\n",
    "    \n",
    "unique_dunn, counts_dunn = np.unique(k_by_dunn, return_counts=True)\n",
    "unique_ch, counts_ch = np.unique(k_by_ch, return_counts=True)\n",
    "unique_db, counts_db = np.unique(k_by_db, return_counts=True)\n",
    "\n",
    "print(\"Best K per Round by Dunn Index:\", dict(zip(unique_dunn, counts_dunn)))\n",
    "print(\"Best K per Round by Calinski-Harabasz Index:\", dict(zip(unique_ch, counts_ch)))\n",
    "print(\"Best K per Round by Davies-Bouldin Index:\", dict(zip(unique_db, counts_db)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cb9b2c4d-4458-47e4-9476-32308854c7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch K-medians\n",
    "def kmedians(data, proto, epochs):\n",
    "    N, p = data.shape\n",
    "    K = proto.shape[0]\n",
    "    SSD = np.zeros(epochs)\n",
    "    \n",
    "    updt_proto = np.copy(proto)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        dist = np.zeros((N, K))\n",
    "        for t in range(N):\n",
    "            dist[t, :] = np.sum(np.abs(data[t, :] - updt_proto), axis=1)\n",
    "        \n",
    "        cluster = np.argmin(dist, axis=1)\n",
    "        SSD[epoch] = np.sum(np.min(dist, axis=1)**2)\n",
    "    \n",
    "        for k in range(K):\n",
    "            I = np.where(cluster == k)[0]\n",
    "            partition = data[I, :]\n",
    "            updt_proto[k, :] = np.median(partition, axis=0)\n",
    "\n",
    "    return updt_proto, SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a66e78d-3eb8-4782-b725-e73ee231dffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.env)",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
